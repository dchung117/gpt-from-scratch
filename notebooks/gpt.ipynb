{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import lzma\n",
    "import tqdm\n",
    "\n",
    "from gpt_builder.tokenizer import Tokenizer\n",
    "from gpt_builder.dataset import BigramDataset\n",
    "from gpt_builder.model.gpt import GPTLanguageModel\n",
    "from gpt_builder.utils import bigram_crossentropy_loss, train_step, get_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare OpenWebText dataset for training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xz_files(data_dir: pathlib.Path) -> list[pathlib.Path]:\n",
    "    \"\"\"\n",
    "    Get list of XZ files in parent directory.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "        data_dir: pathlib.Path\n",
    "            Parent data directory\n",
    "    Returns\n",
    "    -------\n",
    "        list[pathlib.Path]:\n",
    "            List of XZ files\n",
    "    \"\"\"\n",
    "    return [f for f in data_dir.iterdir() if f.is_file() and str(f).endswith(\".xz\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length:  232284\n",
      "DOROTHY AND THE WIZARD IN OZ\n",
      "\n",
      "BY\n",
      "\n",
      "L. FRANK BAUM\n",
      "\n",
      "AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
      "\n",
      "ILLUSTRATED BY JOHN R. NEILL\n",
      "\n",
      "BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW YORK\n",
      "\n",
      "\n",
      "[Illu\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = pathlib.Path(\"../data/openwebtext\")\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "xz_files = get_xz_files(DATA_DIR)\n",
    "train_files = xz_files[:-int(VAL_SPLIT*len(xz_files))]\n",
    "val_files = xz_files[-int(VAL_SPLIT)*len(xz_files):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "vocab = set()\n",
    "\n",
    "with open(DATA_DIR/ f\"output_train.txt\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for j, f_name in enumerate(tqdm(train_files, total=len(train_files))):\n",
    "        with lzma.open(f_name, \"rt\", encoding=\"utf-8\") as in_f:\n",
    "            text = in_f.read()\n",
    "            out_f.write(text)\n",
    "            vocab.update(set(text))\n",
    "\n",
    "with open(DATA_DIR/ f\"output_val.txt\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for j, f_name in enumerate(tqdm(val_files, total=len(val_files))):\n",
    "        with lzma.open(f_name, \"rt\", encoding=\"utf-8\") as in_f:\n",
    "            text = in_f.read()\n",
    "            out_f.write(text)\n",
    "            vocab.update(set(text))\n",
    "\n",
    "with open(DATA_DIR / \"vocab.txt\", \"w\", encoding=\"utf-8\") as vocab_f:\n",
    "    for v in vocab:\n",
    "        vocab_f.write(v+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters:  80\n",
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Get unique characters\n",
    "chars = sorted(set(text))\n",
    "print(\"Number of unique characters: \", len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded hello:  [32, 58, 65, 65, 68]\n",
      "Decoded hello:  Hello\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(chars)\n",
    "hello_tokens = tokenizer.encode(\"Hello\")\n",
    "print(\"Encoded hello: \", hello_tokens)\n",
    "hello_decoded = \"\".join(tokenizer.decode(hello_tokens))\n",
    "print(\"Decoded hello: \", hello_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1, 47, 33,\n",
      "        50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0, 26, 49,  0,  0, 36, 11,\n",
      "         1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,  0, 25, 45, 44, 32, 39,\n",
      "        42,  1, 39, 30,  1, 44, 32, 29,  1, 47, 33, 50, 25, 42, 28,  1, 39, 30,\n",
      "         1, 39, 50,  9,  1, 44, 32, 29,  1, 36, 25, 38, 28,  1, 39, 30,  1, 39,\n",
      "        50,  9,  1, 39, 50, 37, 25,  1, 39, 30,  1, 39, 50,  9,  1, 29, 44, 27,\n",
      "        11,  0,  0, 33, 36, 36, 45, 43, 44, 42, 25, 44, 29, 28,  1, 26, 49,  1,\n",
      "        34, 39, 32, 38,  1, 42, 11,  1, 38, 29, 33, 36, 36,  0,  0, 26, 39, 39,\n",
      "        35, 43,  1, 39, 30,  1, 47, 39, 38, 28, 29, 42,  1, 47, 33, 36, 36, 33,\n",
      "        25, 37,  1, 37, 39, 42, 42, 39, 47,  1,  4,  1, 27, 39, 11,  9,  1, 33,\n",
      "        38, 27, 11,  1, 38, 29, 47,  1, 49, 39, 42, 35,  0,  0,  0, 51, 33, 65,\n",
      "        65, 74])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Wizard of Oz\n",
    "data = tokenizer.encode(text, return_tensors=True)\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bigram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In bigram:  tensor([28, 39, 42, 39, 44, 32, 49,  1])\n",
      "Out bigram:  tensor([39, 42, 39, 44, 32, 49,  1, 25])\n"
     ]
    }
   ],
   "source": [
    "dataset = BigramDataset(data)\n",
    "in_bigram, out_bigram = dataset[0]\n",
    "print(\"In bigram: \", in_bigram)\n",
    "print(\"Out bigram: \", out_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m GPTLanguageModel(vocab_size, block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m x_out \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_bigram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      5\u001b[0m x_out\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt-from-scratch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt-from-scratch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gpt-from-scratch/src/gpt_builder/model/gpt.py:61\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[0;34m(self, idxs)\u001b[0m\n\u001b[1;32m     58\u001b[0m x_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(idxs)  \u001b[38;5;66;03m# (B, T, d_embed)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m x_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size, device\u001b[38;5;241m=\u001b[39mx_tokens\u001b[38;5;241m.\u001b[39mdevice))  \u001b[38;5;66;03m# (T, d_embed)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_pos\u001b[49m  \u001b[38;5;66;03m# (B, T, d_embed)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders(x)  \u001b[38;5;66;03m# (B, T, d_embed)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)  \u001b[38;5;66;03m# (B, T, d_embed)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm = GPTLanguageModel(vocab_size, block_size=64, n_heads=8, n_decoders=8).to(device)\n",
    "x_out = llm(in_bigram.to(device)).cpu()\n",
    "x_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2320, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cros entropy lloss\n",
    "bigram_crossentropy_loss(x_out, out_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new tokens\n",
    "llm.eval()\n",
    "x_new = llm.generate(in_bigram.to(device), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  ['D', 'O', 'R', 'O', 'T', 'H', 'Y', ' ']\n",
      "New sequence:  ['D', 'O', 'R', 'O', 'T', 'H', 'Y', ' ', 'O', '[', 'B', 'Y', '\\n', 'L', '.', '.', ' ', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Decode new sequence\n",
    "print(\"Context: \", tokenizer.decode(in_bigram.tolist()))\n",
    "print(\"New sequence: \", tokenizer.decode(x_new[0].cpu().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step: 0\n",
      "Training loss: 4.728\n",
      "Test loss: 4.721\n",
      "\n",
      "Train step: 250\n",
      "Training loss: 4.728\n",
      "Test loss: 4.720\n",
      "\n",
      "Train step: 500\n",
      "Training loss: 4.727\n",
      "Test loss: 4.719\n",
      "\n",
      "Train step: 750\n",
      "Training loss: 4.726\n",
      "Test loss: 4.719\n",
      "\n",
      "Final training loss:  4.629518508911133\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=128\n",
    "N_ITERS =1000\n",
    "LEARNING_RATE = 3e-4\n",
    "TEST_SPLIT = 0.2\n",
    "EVAL_ITERS = 100\n",
    "\n",
    "n_test = int(len(data)*TEST_SPLIT)\n",
    "train_dataset = BigramDataset(data[:-n_test])\n",
    "test_dataset = BigramDataset(data[-n_test:])\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=128)\n",
    "test_dl = DataLoader(test_dataset, batch_size=128)\n",
    "optim = torch.optim.AdamW(llm.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for i in range(N_ITERS):\n",
    "    inputs, targets = next(iter(train_dl))\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    if i % EVAL_ITERS == 0:\n",
    "        train_loss = get_loss(train_dl, llm, device)\n",
    "        test_loss = get_loss(test_dl, llm, device)\n",
    "        print(f\"Train step: {i}\")\n",
    "        print(f\"Training loss: {train_loss:.3f}\")\n",
    "        print(f\"Test loss: {test_loss:.3f}\")\n",
    "        print()\n",
    "    loss = train_step(llm, inputs, targets, optim)\n",
    "\n",
    "print(\"Final training loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CONTEXT\n",
      ", with drawn gravey poured over it.\n",
      "\n",
      "\"Fish!\" cried Jim, with a sniff. \"Do you take me for a tom-cat?\n",
      "\n",
      "TEST GENERATED\n",
      ", with drawn gravey poured over it.\n",
      "\n",
      "\"Fish!\" cried Jim, with a sniff. \"Do you take me for a tom-cat?\"Z\n",
      "o]Ua\n",
      "&4yD WI,D ITyM\n",
      "n0dPZMoyuvHY WIuCND(UTHY\n",
      "AUM\n",
      "Xg-FRAUTH*IsXeHBY\n",
      "AU2nOF WIZARAUTHEb9[?Y BARD WI\n"
     ]
    }
   ],
   "source": [
    "in_test = data[-n_test:-n_test+100]\n",
    "llm.eval()\n",
    "pred_test = llm.generate(in_test.to(device), 100).cpu()[0]\n",
    "\n",
    "print(\"TEST CONTEXT\")\n",
    "print(\"\".join(tokenizer.decode(in_test.tolist())))\n",
    "print()\n",
    "print(\"TEST GENERATED\")\n",
    "print(\"\".join(tokenizer.decode(pred_test.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
