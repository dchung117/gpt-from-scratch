{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length:  232284\n",
      "DOROTHY AND THE WIZARD IN OZ\n",
      "\n",
      "BY\n",
      "\n",
      "L. FRANK BAUM\n",
      "\n",
      "AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
      "\n",
      "ILLUSTRATED BY JOHN R. NEILL\n",
      "\n",
      "BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW YORK\n",
      "\n",
      "\n",
      "[Illu\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path(\"data\")\n",
    "with open(data_dir / \"wizard_of_oz.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(\"Text length: \", len(text))\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters:  80\n",
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Get unique characters\n",
    "chars = sorted(set(text))\n",
    "print(\"Number of unique characters: \", len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \"\"\"\n",
    "    Tokenizer constructor from provided vocabulary.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        str_to_int: dict[str, int]\n",
    "            Mapping from string in vocabulary to integer\n",
    "        int_to_str: dict[int, str]\n",
    "            Mapping from integer to string in vocabulary\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "        encode[strs] -> list[int]\n",
    "        decode[tokens] -> list[strs]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab: list[str]) -> None:\n",
    "        self.str_to_int = {c:i for i,c in enumerate(vocab)}\n",
    "        self.int_to_str = {i:c for i,c in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, strs: list[str]) -> list[int]:\n",
    "        \"\"\"\n",
    "        Encode a list of strings into a list of tokens.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "            strs: list[str]\n",
    "                List of strings\n",
    "        Return\n",
    "        ------\n",
    "            list[int]:\n",
    "                List of encoded strings\n",
    "        \"\"\"\n",
    "        return [self.str_to_int[c] for c in strs]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Decode a list of tokens into a list of strings.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "            tokens: list[int]\n",
    "                List of tokens\n",
    "        Return\n",
    "        ------\n",
    "            list[str]\n",
    "                List of decoded tokens\n",
    "        \"\"\"\n",
    "        return [self.int_to_str[i] for i in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
