{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from gpt_builder.tokenizer import Tokenizer\n",
    "from gpt_builder.dataset import BigramDataset\n",
    "from gpt_builder.model import BigramLLM\n",
    "from gpt_builder.utils import bigram_crossentropy_loss, train_step, get_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length:  232284\n",
      "DOROTHY AND THE WIZARD IN OZ\n",
      "\n",
      "BY\n",
      "\n",
      "L. FRANK BAUM\n",
      "\n",
      "AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
      "\n",
      "ILLUSTRATED BY JOHN R. NEILL\n",
      "\n",
      "BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW YORK\n",
      "\n",
      "\n",
      "[Illu\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path(\"data\")\n",
    "with open(data_dir / \"wizard_of_oz.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(\"Text length: \", len(text))\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters:  80\n",
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Get unique characters\n",
    "chars = sorted(set(text))\n",
    "print(\"Number of unique characters: \", len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded hello:  [32, 58, 65, 65, 68]\n",
      "Decoded hello:  Hello\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(chars)\n",
    "hello_tokens = tokenizer.encode(\"Hello\")\n",
    "print(\"Encoded hello: \", hello_tokens)\n",
    "hello_decoded = \"\".join(tokenizer.decode(hello_tokens))\n",
    "print(\"Decoded hello: \", hello_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1, 47, 33,\n",
      "        50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0, 26, 49,  0,  0, 36, 11,\n",
      "         1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,  0, 25, 45, 44, 32, 39,\n",
      "        42,  1, 39, 30,  1, 44, 32, 29,  1, 47, 33, 50, 25, 42, 28,  1, 39, 30,\n",
      "         1, 39, 50,  9,  1, 44, 32, 29,  1, 36, 25, 38, 28,  1, 39, 30,  1, 39,\n",
      "        50,  9,  1, 39, 50, 37, 25,  1, 39, 30,  1, 39, 50,  9,  1, 29, 44, 27,\n",
      "        11,  0,  0, 33, 36, 36, 45, 43, 44, 42, 25, 44, 29, 28,  1, 26, 49,  1,\n",
      "        34, 39, 32, 38,  1, 42, 11,  1, 38, 29, 33, 36, 36,  0,  0, 26, 39, 39,\n",
      "        35, 43,  1, 39, 30,  1, 47, 39, 38, 28, 29, 42,  1, 47, 33, 36, 36, 33,\n",
      "        25, 37,  1, 37, 39, 42, 42, 39, 47,  1,  4,  1, 27, 39, 11,  9,  1, 33,\n",
      "        38, 27, 11,  1, 38, 29, 47,  1, 49, 39, 42, 35,  0,  0,  0, 51, 33, 65,\n",
      "        65, 74])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Wizard of Oz\n",
    "data = tokenizer.encode(text, return_tensors=True)\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bigram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In bigram:  tensor([28, 39, 42, 39, 44, 32, 49,  1])\n",
      "Out bigram:  tensor([39, 42, 39, 44, 32, 49,  1, 25])\n"
     ]
    }
   ],
   "source": [
    "dataset = BigramDataset(data)\n",
    "in_bigram, out_bigram = dataset[0]\n",
    "print(\"In bigram: \", in_bigram)\n",
    "print(\"Out bigram: \", out_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 80])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm = BigramLLM(vocab_size).to(device)\n",
    "x_out = llm(in_bigram.to(device)).cpu()\n",
    "x_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2320, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cros entropy lloss\n",
    "bigram_crossentropy_loss(x_out, out_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new tokens\n",
    "llm.eval()\n",
    "x_new = llm.generate(in_bigram.to(device), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  ['D', 'O', 'R', 'O', 'T', 'H', 'Y', ' ']\n",
      "New sequence:  ['D', 'O', 'R', 'O', 'T', 'H', 'Y', ' ', 'O', '[', 'B', 'Y', '\\n', 'L', '.', '.', ' ', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Decode new sequence\n",
    "print(\"Context: \", tokenizer.decode(in_bigram.tolist()))\n",
    "print(\"New sequence: \", tokenizer.decode(x_new[0].cpu().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step: 0\n",
      "Training loss: 4.919087\n",
      "Test loss: 4.926863\n",
      "\n",
      "Train step: 250\n",
      "Training loss: 4.917075\n",
      "Test loss: 4.924778\n",
      "\n",
      "Train step: 500\n",
      "Training loss: 4.915348\n",
      "Test loss: 4.922974\n",
      "\n",
      "Train step: 750\n",
      "Training loss: 4.913933\n",
      "Test loss: 4.921480\n",
      "\n",
      "Train step: 1000\n",
      "Training loss: 4.912862\n",
      "Test loss: 4.920325\n",
      "\n",
      "Train step: 1250\n",
      "Training loss: 4.912166\n",
      "Test loss: 4.919541\n",
      "\n",
      "Train step: 1500\n",
      "Training loss: 4.911876\n",
      "Test loss: 4.919157\n",
      "\n",
      "Train step: 1750\n",
      "Training loss: 4.912023\n",
      "Test loss: 4.919205\n",
      "\n",
      "Train step: 2000\n",
      "Training loss: 4.912633\n",
      "Test loss: 4.919711\n",
      "\n",
      "Train step: 2250\n",
      "Training loss: 4.913730\n",
      "Test loss: 4.920697\n",
      "\n",
      "Train step: 2500\n",
      "Training loss: 4.915331\n",
      "Test loss: 4.922182\n",
      "\n",
      "Train step: 2750\n",
      "Training loss: 4.917449\n",
      "Test loss: 4.924178\n",
      "\n",
      "Train step: 3000\n",
      "Training loss: 4.920085\n",
      "Test loss: 4.926686\n",
      "\n",
      "Train step: 3250\n",
      "Training loss: 4.923234\n",
      "Test loss: 4.929701\n",
      "\n",
      "Train step: 3500\n",
      "Training loss: 4.926878\n",
      "Test loss: 4.933208\n",
      "\n",
      "Train step: 3750\n",
      "Training loss: 4.930994\n",
      "Test loss: 4.937181\n",
      "\n",
      "Train step: 4000\n",
      "Training loss: 4.935548\n",
      "Test loss: 4.941590\n",
      "\n",
      "Train step: 4250\n",
      "Training loss: 4.940505\n",
      "Test loss: 4.946399\n",
      "\n",
      "Train step: 4500\n",
      "Training loss: 4.945829\n",
      "Test loss: 4.951574\n",
      "\n",
      "Train step: 4750\n",
      "Training loss: 4.951492\n",
      "Test loss: 4.957087\n",
      "\n",
      "Train step: 5000\n",
      "Training loss: 4.957471\n",
      "Test loss: 4.962914\n",
      "\n",
      "Train step: 5250\n",
      "Training loss: 4.963748\n",
      "Test loss: 4.969040\n",
      "\n",
      "Train step: 5500\n",
      "Training loss: 4.970314\n",
      "Test loss: 4.975454\n",
      "\n",
      "Train step: 5750\n",
      "Training loss: 4.977157\n",
      "Test loss: 4.982145\n",
      "\n",
      "Train step: 6000\n",
      "Training loss: 4.984264\n",
      "Test loss: 4.989101\n",
      "\n",
      "Train step: 6250\n",
      "Training loss: 4.991621\n",
      "Test loss: 4.996307\n",
      "\n",
      "Train step: 6500\n",
      "Training loss: 4.999205\n",
      "Test loss: 5.003743\n",
      "\n",
      "Train step: 6750\n",
      "Training loss: 5.006991\n",
      "Test loss: 5.011384\n",
      "\n",
      "Train step: 7000\n",
      "Training loss: 5.014954\n",
      "Test loss: 5.019206\n",
      "\n",
      "Train step: 7250\n",
      "Training loss: 5.023071\n",
      "Test loss: 5.027185\n",
      "\n",
      "Train step: 7500\n",
      "Training loss: 5.031328\n",
      "Test loss: 5.035309\n",
      "\n",
      "Train step: 7750\n",
      "Training loss: 5.039723\n",
      "Test loss: 5.043576\n",
      "\n",
      "Train step: 8000\n",
      "Training loss: 5.048263\n",
      "Test loss: 5.051991\n",
      "\n",
      "Train step: 8250\n",
      "Training loss: 5.056964\n",
      "Test loss: 5.060568\n",
      "\n",
      "Train step: 8500\n",
      "Training loss: 5.065833\n",
      "Test loss: 5.069316\n",
      "\n",
      "Train step: 8750\n",
      "Training loss: 5.074876\n",
      "Test loss: 5.078239\n",
      "\n",
      "Train step: 9000\n",
      "Training loss: 5.084083\n",
      "Test loss: 5.087327\n",
      "\n",
      "Train step: 9250\n",
      "Training loss: 5.093441\n",
      "Test loss: 5.096567\n",
      "\n",
      "Train step: 9500\n",
      "Training loss: 5.102927\n",
      "Test loss: 5.105936\n",
      "\n",
      "Train step: 9750\n",
      "Training loss: 5.112518\n",
      "Test loss: 5.115409\n",
      "\n",
      "Final training loss:  1.2522989511489868\n"
     ]
    }
   ],
   "source": [
    "N_ITERS =10_000\n",
    "LEARNING_RATE = 3e-4\n",
    "TEST_SPLIT = 0.2\n",
    "EVAL_ITERS = 250\n",
    "\n",
    "n_test = int(len(data)*TEST_SPLIT)\n",
    "train_dataset = BigramDataset(data[:-n_test])\n",
    "test_dataset = BigramDataset(data[-n_test:])\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=64)\n",
    "test_dl = DataLoader(test_dataset, batch_size=64)\n",
    "optim = torch.optim.AdamW(llm.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for i in range(N_ITERS):\n",
    "    inputs, targets = next(iter(train_dl))\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    if i % EVAL_ITERS == 0:\n",
    "        train_loss = get_loss(train_dl, llm, device)\n",
    "        test_loss = get_loss(test_dl, llm, device)\n",
    "        print(f\"Train step: {i}\")\n",
    "        print(f\"Training loss: {train_loss:3f}\")\n",
    "        print(f\"Test loss: {test_loss:3f}\")\n",
    "        print()\n",
    "    loss = train_step(llm, inputs, targets, optim)\n",
    "\n",
    "print(\"Final training loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CONTEXT\n",
      ", with drawn gravey poured over it.\n",
      "\n",
      "\"Fish!\" cried Jim, with a sniff. \"Do you take me for a tom-cat?\n",
      "\n",
      "TEST GENERATED\n",
      ", with drawn gravey poured over it.\n",
      "\n",
      "\"Fish!\" cried Jim, with a sniff. \"Do you take me for a tom-cat?\"Z\n",
      "o]Ua\n",
      "&4yD WI,D ITyM\n",
      "n0dPZMoyuvHY WIuCND(UTHY\n",
      "AUM\n",
      "Xg-FRAUTH*IsXeHBY\n",
      "AU2nOF WIZARAUTHEb9[?Y BARD WI\n"
     ]
    }
   ],
   "source": [
    "in_test = data[-n_test:-n_test+100]\n",
    "llm.eval()\n",
    "pred_test = llm.generate(in_test.to(device), 100).cpu()[0]\n",
    "\n",
    "print(\"TEST CONTEXT\")\n",
    "print(\"\".join(tokenizer.decode(in_test.tolist())))\n",
    "print()\n",
    "print(\"TEST GENERATED\")\n",
    "print(\"\".join(tokenizer.decode(pred_test.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
